{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d1d15d-94e3-4ab0-a62e-47a4c0f3defc",
   "metadata": {},
   "source": [
    "# Learned Inertial Odometry\n",
    "\n",
    "The paper by G. Cioffi et al. in RA-L 2023 claims that the IMU-based pose estimate $\\vec{p}$ of a flying drone can be improved with the help of a learning-based model that estimates the relative pose change $\\Delta \\vec{p}$ between the current and previous time steps and known thrust (force) commands. Their code is available at\n",
    "\n",
    " * https://github.com/uzh-rpg/learned_inertial_model_odometry/\n",
    "\n",
    "The idea is good and intuitive, but their solution raises a few questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e90d6dc-d2c2-44f7-9bd4-11002748ab3f",
   "metadata": {},
   "source": [
    "## Concerns about the solution\n",
    "\n",
    "In principle, the actions (thrust commands) of a drone define how it moves and thus can provide an estimate of the drone's pose $\\vec{p}_t$ at any time instant $t$. Such mapping from a sequence of action commands to the current pose can be learned, for example, using a neural network regressor. However, this requires that i) the starting conditions are the same (e.g. the drone in the same pose on the ground) and ii) the action sequence contains all actions from the beginning.\n",
    "\n",
    "**Relative pose paradox**\n",
    "\n",
    "Why Cioffi et al. do not estimate the actual pose given the executed actions (thrust commands) but only a relative pose?\n",
    "\n",
    "One reason could be, that they want to avoid difficulties related to arbitrary length sequences, which would need using recurrent neural networks (RNNs) that are often difficult to train. Instead, they use only $T$ latest actions (thrust commands). Naturally, one cannot know the absolute pose if only partial information about \"movements\" is known. However, it sounds intuitively correct that the relative change occurred between the latest $a_T$ and the previous action $a_{T-1}$ can be estimated. However, again a few questions arise:\n",
    "\n",
    " * Why multiple previous actions $a_T, a_{T-1}, a_{T-2}, \\ldots, a_i$ improve the estimate at time $t=T$ if not all of them are available anyway - estimator should need only the latest action $a_T$ for relative change, right?\n",
    " * If a sequence of previous actions are used, then why not all of them as obviously this information is needed (or above is correct) and in this case some information is lost?\n",
    "\n",
    "Actually, in their work the system runs at 100Hz and the history buffer is 0.5 seconds so the number of samples T=50. That does not sound that much so again another question raises\n",
    "\n",
    " * Why not to use a vanilla multilayer perceptron (MLP) instead of the more complex \"temporal convolutional network\" (TCN) from this [webpage](https://github.com/locuslab/TCN)?\n",
    "\n",
    "It is assumed that TCN becomes useful when sequences are truly long such as 10,000 or 100,000 samples and in the TCN paper the performance is not that much better than standard RNNs - so why all this hassle?\n",
    "\n",
    "**Even the relative pose estimation needs the full action history**\n",
    "\n",
    "What makes the proposed solution even more questionable is that *under unknown physics it cannot be guaranteed that relative pose can be estimated from incomplete history of actions!*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb7a859-e387-4cb0-a7d7-b21ec28bddb1",
   "metadata": {},
   "source": [
    "## Proof of the claim\n",
    "To illustrate the above claim that full action sequence is needed OR at least information about the sequence such as its length, we run some experiments on the simple Gymnasium environment *Pendulum*\n",
    "\n",
    "<div><img src=\"images/pendulum.png\"/></div>\n",
    "\n",
    "for which the description is available at the Gymnasium [homepage](https://gymnasium.farama.org/environments/classic_control/pendulum/). However, these experiments should be easy to replicate with any other Gym environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e950c10-cabf-44c3-835c-b353312a9065",
   "metadata": {},
   "source": [
    "Create the environment - in this case we assume that the current observation (the state of the environment) is the pose to be estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81b2e2-8aa7-4c87-a3c2-08eeac543608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "env = gym.make('Pendulum-v1', g=9.81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b4ebaf-2cc4-4d17-9031-1625607070a0",
   "metadata": {},
   "source": [
    "### Experiment 1: Train MLP using action-pose (observation) pairs\n",
    "\n",
    "Our model is single input - single output model $\\vec{p}_t = f(a_t)$ i.e. given the latest action, estimate the current absolute pose.\n",
    "\n",
    "This is clearly impossible in many cases. For example, assume that the only action is either 0:\"do nothing\" or 1:\"turn 5 degrees right\". We cannot know the absolute pose (e.g. \"pointing toward 25 degrees to the right\") if we don't know that the previous four commands where (1,1,1,1).\n",
    "\n",
    "However, let's verify this experimentally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61b45e8-f538-4455-9b9a-44b3bdc49c16",
   "metadata": {},
   "source": [
    "Collect training data of N samples. If the environment is terminated, we just restart it (it starts from an arbitrary initial conditions). Actions are 1-dimensional and state p is 3-dimensional.\n",
    "\n",
    "**Note:** the pose p(T) corresponds to pose after action a(T) so the initial pose is missing (stored in p_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db790ee9-507c-423c-a306-3268aa01b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_tr = 1000\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "p_0 = observation\n",
    "a_l = np.empty((N_tr,1))\n",
    "p_l = np.empty((N_tr,3))\n",
    "\n",
    "num_of_e = 1\n",
    "for s in range(N_tr):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    action = [0]\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    a_l[s] = action\n",
    "    p_l[s,:] = observation\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        num_of_e += 1\n",
    "print(f'Contains data from {num_of_e} episodes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604c2652-3f3b-4060-b69a-2b0cf1e1f6a4",
   "metadata": {},
   "source": [
    "Construct a simple MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a95424-8ee4-4dd6-ad68-0ace6d896606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0966826-41dc-448a-badb-bf3165f10f32",
   "metadata": {},
   "source": [
    "Train the model for some epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a34236-9284-44c4-b022-069f426abbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(a_l,p_l, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde64af1-9432-4f2b-ac4e-60bd31275906",
   "metadata": {},
   "source": [
    "Plot the loss to make sure model is learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660b60f3-3445-4170-a9ec-24e34cdf8488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02167b77-044f-4ac8-a838-6ed5904906b7",
   "metadata": {},
   "source": [
    "Run one episode and store N_te samples for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153dfc3a-3f0c-4758-9fb6-6960c98354fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_te = 100\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "p_0_test = observation \n",
    "a_l_test = np.empty((N_te,1))\n",
    "p_l_test = np.empty((N_te,3))\n",
    "\n",
    "for s in range(N_te):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    a_l_test[s] = action\n",
    "    p_l_test[s,:] = observation\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f'{s+1} samples generated for testing')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2061e89e-2723-4db6-9121-0c3ee32e1e6d",
   "metadata": {},
   "source": [
    "Compute predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7ff6b8-e215-4f93-9e0a-54a1e197a303",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hat = model.predict(a_l_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0b6d4c-73fd-49a1-bf3b-91d615aa6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(range(s), p_l_test[:s,0], range(s), p_hat[:s,0])\n",
    "plt.title('x')\n",
    "plt.legend(['gt','predicted'])\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('y')\n",
    "plt.plot(range(s), p_l_test[:s,1], range(s), p_hat[:s,1])\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('Angular velocity')\n",
    "plt.plot(range(s), p_l_test[:s,2], range(s), p_hat[:s,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1366ba2d-072e-4663-9e15-dce3d707cec3",
   "metadata": {},
   "source": [
    "The results show that nothing works. The estimated pose cannot be correct since\n",
    "\n",
    " * Not even the first estimate cannot be correct as the environment always restarts from random pose\n",
    " * All history of actions is lost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb30471-da26-49ad-8056-deea0ac956b0",
   "metadata": {},
   "source": [
    "## Experiment 2: Train MLP using action - relative pose change\n",
    "\n",
    "Learning $\\Delta p_t = f(a_t)$ makes more sense since intuitively the current action affects a \"change\" to the current pose. The current pose can then be estimated by summing all relative pose changes to the initial pose $p = p_0+\\sum_t \\Delta p_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521b5d79-22f1-4e5c-9733-10d93b59c823",
   "metadata": {},
   "source": [
    "Construct training data of relative poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb928e69-47d2-4ba4-bb48-586b62c8ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_tr = 1000\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "p_0 = observation\n",
    "a_l = np.empty((N_tr,1))\n",
    "p_l = np.empty((N_tr,3))\n",
    "p_delta_l = np.empty((N_tr,3))\n",
    "\n",
    "prev_observation = observation\n",
    "num_of_e = 1\n",
    "for s in range(N_tr):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    action = [0]\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    a_l[s] = action\n",
    "    p_l[s,:] = observation\n",
    "    p_delta_l[s,:] = observation-prev_observation\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        num_of_e += 1\n",
    "    prev_observation = observation\n",
    "print(f'Contains data from {num_of_e} episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36833d56-903c-49b9-ad30-ebe37cd954c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1c93b-b983-4a38-8c3a-426828add0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(a_l,p_delta_l, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0fab59-0ba9-4c28-864d-d2dc1f5cc7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e51e4-2048-4f12-a045-c8a116bd88d9",
   "metadata": {},
   "source": [
    "Run episode for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0bfb6f-bf7d-4ccc-b3a5-717f21ffba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_te = 100\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "p_0_test = observation \n",
    "a_l_test = np.empty((N_te,1))\n",
    "p_l_test = np.empty((N_te,3))\n",
    "\n",
    "for s in range(N_te):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    a_l_test[s] = action\n",
    "    p_l_test[s,:] = observation\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f'{s+1} samples generated for testing')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df85ede-9e49-4e42-a953-60ac908caee2",
   "metadata": {},
   "source": [
    "Add the starting point and compute cumulative sum of deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3e7a9-4bb7-4ce5-8c04-b839e9eb650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_delta_hat = model.predict(a_l_test)\n",
    "p_hat = p_0_test+np.cumsum(p_delta_hat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81fce76-61cb-4107-9976-4d60fc1cb6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "plt.plot(range(s), p_l_test[:s,0], range(s), p_hat[:s,0])\n",
    "plt.title('x')\n",
    "plt.legend(['gt','predicted'])\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('y')\n",
    "plt.plot(range(s), p_l_test[:s,1], range(s), p_hat[:s,1])\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('Angular velocity')\n",
    "plt.plot(range(s), p_l_test[:s,2], range(s), p_hat[:s,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fe5da6-4fbf-413b-8f90-7b546558289b",
   "metadata": {},
   "source": [
    "Results show that the first 1-2 estimates are close to the starting point, but otherwise estimates are wrong. For example, what happens next is very different for a pendulum swinging from left to right than right to left. And the first action has very little effect to that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0c758b-32ca-4f55-9f6f-4e91971d2992",
   "metadata": {},
   "source": [
    "### Experiment 3: Fixing the starting point for delta pose estimation\n",
    "\n",
    "This starts to make sense, since the same actions have always the same effect if we always start from the same pose.\n",
    "\n",
    "**Note:** Instead of long sequences, let's use only short sequences as actual pose depends on action history and this still does not have idea what the previous actions have been."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54c6fc-27e2-4ef4-8c44-2a77794eb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 42\n",
    "N_tr = 1000\n",
    "T_max = 5 # This many steps at most in every episode\n",
    "\n",
    "observation, info = env.reset(seed=rand_seed)\n",
    "\n",
    "p_0 = observation\n",
    "a_l = np.empty((N_tr,1))\n",
    "p_l = np.empty((N_tr,3))\n",
    "p_delta_l = np.empty((N_tr,3))\n",
    "\n",
    "prev_observation = observation\n",
    "num_of_e = 1\n",
    "for s in range(N_tr):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    action = [0]\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    a_l[s] = action\n",
    "    p_l[s,:] = observation\n",
    "    p_delta_l[s,:] = observation-prev_observation\n",
    "    \n",
    "    if terminated or truncated or (s % T_max) == 0:\n",
    "        observation, info = env.reset(seed=rand_seed)\n",
    "        num_of_e += 1\n",
    "    prev_observation = observation\n",
    "print(f'Contains data from {num_of_e} episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0aec67e-99a5-4567-9490-76329b62ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b47760-c5fa-45d8-aeaf-98206404b454",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(a_l,p_delta_l, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65597f41-dbb6-4d17-afd2-e854ec8ec7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca96e33-1db0-44bf-b99a-19e2fda0ffb7",
   "metadata": {},
   "source": [
    "Generate training data\n",
    "\n",
    "**Note:** You must generate several times as sometimes the result is good and sometimes not (note the randomness in action selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9396d6-653f-404b-8403-541b0142fcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_te = 50\n",
    "\n",
    "observation, info = env.reset(seed=rand_seed)\n",
    "\n",
    "p_0_test = observation \n",
    "a_l_test = np.empty((N_te,1))\n",
    "p_l_test = np.empty((N_te,3))\n",
    "\n",
    "for s in range(N_te):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    a_l_test[s] = action\n",
    "    p_l_test[s,:] = observation\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f'{s+1} samples generated for testing')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb43cd-983e-43b3-9ab8-137b30140421",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_delta_hat = model.predict(a_l_test)\n",
    "p_hat = p_0_test+np.cumsum(p_delta_hat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a52376-dcbe-4c17-b111-9dc89c4e1669",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "plt.plot(range(s), p_l_test[:s,0], range(s), p_hat[:s,0])\n",
    "plt.title('x')\n",
    "plt.legend(['gt','predicted'])\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('y')\n",
    "plt.plot(range(s), p_l_test[:s,1], range(s), p_hat[:s,1])\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('Angular velocity')\n",
    "plt.plot(range(s), p_l_test[:s,2], range(s), p_hat[:s,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4273b4e6-a030-4e2d-bc10-fdad636d1a6e",
   "metadata": {},
   "source": [
    "Now the estimates start to look more meaningful. Since the pose always starts from the same initial pose, the first estimates are close to correct. However, the quality degrades the longer time from the beginning. This is because the estimate does not have an idea how many time steps have been taken (the longer time the more random actions). Therefore it is also difficult to make this predictor work well if the sequence lenght substantially increases.\n",
    "\n",
    "**Note:** This would improve the more history inputs added, BUT that only moves drifting away further, not solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3700cd8b-31c2-48e8-a94f-491698c0385c",
   "metadata": {},
   "source": [
    "### Experiment 4: Giving the predictor sense of time\n",
    "\n",
    "The accumulation of time should aid the predictor to estimate what has happened so far and thus be much better in predicting how the current action will change the pose. Note that this cannot be accurate as actions have been random so that also affects to the current pose and also what happens next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534a581-14f4-4050-b696-01e292d288d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = 42\n",
    "N_tr = 2000\n",
    "T_max = 20 # This many steps at most in every episode\n",
    "\n",
    "observation, info = env.reset(seed=rand_seed)\n",
    "\n",
    "p_0 = observation\n",
    "a_l = np.empty((N_tr,1))\n",
    "p_l = np.empty((N_tr,3))\n",
    "p_delta_l = np.empty((N_tr,3))\n",
    "t_l = np.empty((N_tr,1))\n",
    "\n",
    "prev_observation = observation\n",
    "num_of_e = 1\n",
    "steps = 0\n",
    "for s in range(N_tr):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    action = [0]\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    a_l[s] = action\n",
    "    p_l[s,:] = observation\n",
    "    p_delta_l[s,:] = observation-prev_observation\n",
    "    t_l[s] = steps\n",
    "    \n",
    "    if terminated or truncated or (s % T_max) == 0:\n",
    "        observation, info = env.reset(seed=rand_seed)\n",
    "        num_of_e += 1\n",
    "        steps = 0\n",
    "    else:\n",
    "        steps += 1\n",
    "        \n",
    "    prev_observation = observation\n",
    "print(f'Contains data from {num_of_e} episodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b39b4-c520-4ee4-b16c-dae963289718",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Dense(64, activation='sigmoid'),\n",
    "  tf.keras.layers.Dense(3)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ddf513-9f7f-4084-8f1e-e931bc47dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([a_l,t_l])\n",
    "\n",
    "history = model.fit(X,p_delta_l, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74cf343-838a-4414-b6f2-23d5297633f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3e7b9-b36c-4016-9b1a-d7e459e3b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_te = 50\n",
    "\n",
    "observation, info = env.reset(seed=rand_seed)\n",
    "\n",
    "p_0_test = observation \n",
    "a_l_test = np.empty((N_te,1))\n",
    "p_l_test = np.empty((N_te,3))\n",
    "t_l_test = np.empty((N_te,1))\n",
    "\n",
    "steps = 0\n",
    "for s in range(N_te):\n",
    "    action = env.action_space.sample()  # agent policy that uses the observation and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    a_l_test[s] = action\n",
    "    p_l_test[s,:] = observation\n",
    "    t_l_test[s] = steps\n",
    "\n",
    "    steps += 1\n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(f'{s+1} samples generated for testing')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b89074-8a7d-49b4-bdfa-339fe2dcc8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te = np.hstack([a_l_test,t_l_test])\n",
    "\n",
    "p_delta_hat = model.predict(X_te)\n",
    "p_hat = p_0_test+np.cumsum(p_delta_hat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc2597b-61bf-4bb8-9d0c-f07a2ac10c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,3,1)\n",
    "plt.plot(range(s), p_l_test[:s,0], range(s), p_hat[:s,0])\n",
    "plt.title('x')\n",
    "plt.legend(['gt','predicted'])\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('y')\n",
    "plt.plot(range(s), p_l_test[:s,1], range(s), p_hat[:s,1])\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('Angular velocity')\n",
    "plt.plot(range(s), p_l_test[:s,2], range(s), p_hat[:s,2])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
