{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69aa7a6d-c26d-4f74-bbb9-2d10a193a6aa",
   "metadata": {},
   "source": [
    "# Time series forecasting using PyTorch Transformer\n",
    "\n",
    "The goal is to predict future sequence values (real numbers) in generative manner. To be more precise, there are known past values $\\mathbf{X} = \\{x_0, x_1, \\ldots , x_N\\}$ (inputs) and the model predicts future values $\\mathbf{Y} = \\{\\hat{x}_{N+1}, \\hat{x}_{N+2}, \\ldots , \\hat{x}_{N+M}\\}$. PyTorch Transformer is adopted since it should be ideal for the task. Transformer is used to generate the new values by iteratively using the known values and already generated samples:\n",
    "\n",
    " * $y_1$ = Transformer($\\mathbf{X},y_0$)\n",
    " * $y_2$ = Transformer($\\mathbf{X},y_0, y_1$)\n",
    " * $y_3$ = Transformer($\\mathbf{X},y_0, y_1, y_2$)\n",
    "\n",
    "New symbol $y_i$ is used instead of $\\hat{x}_{N+j}$ to more explicitly discriminate between the known and generated sequences. The first prediction, $y_0$, is often the *start of sequence* symbol `<SOS>`.\n",
    "\n",
    "**Transformer** Transformer is a neural architecture that is considered as the golden standard for sequence-to-sequence modeling. However, thanks to ChatGPT and other large language models (LLMs) most texts about programming with transformers are related to natural language processing (NLP). Another branch are texts about how to implement Transformer from the scratch. There are less posts about how to use the existing Transformer models for generic real valued data prediction instead of language.\n",
    "\n",
    "In the following, we build the final model step by step starting from *SimplestTransformer* to *TimeSeriesTransformer*. The covered steps add one at time of the essential building blocks:\n",
    "\n",
    " * Tokenization\n",
    " * Embedding and unembedding\n",
    " * Masked attention\n",
    " * Position encoding\n",
    " * Quantized real value embedding\n",
    " * Direct real value prediction\n",
    "\n",
    "For more information about them, check the Transformer Wiki page: https://en.wikipedia.org/wiki/Transformer_(deep_learning_architecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539a705-119c-4e71-8946-44fba9329409",
   "metadata": {},
   "source": [
    "## Torch.nn.Transformer\n",
    "\n",
    "It seems that the generic Transformer module of PyTorch is becoming depricated, but since it is used in many example codes it is used as the main module.\n",
    "\n",
    "Fundamental imports are the following and this code is tested on Python 3.12.2 and everything installed through Linux Conda package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb6886dc-d8dd-40b6-9570-55f6d10016b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import platform\n",
    "\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b2353f-7deb-4136-9c19-fb9928a5e120",
   "metadata": {},
   "source": [
    "## SimplestTransformer - As simpe as possible and what we can do with it\n",
    "\n",
    "What can we do with the simplest possible transformer architecture that does not include anything else but the transformer itself.\n",
    "\n",
    "Let's construct a SimplestTransformer class that has [nn.torch.Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) as member.\n",
    "\n",
    "The following model takes a sequence (e.g. [0, 0, 0, 0]) as input, puts it through torch.nn.Transformer gives its outputs directly as output.\n",
    "\n",
    "**Note:** The only additional change to the default parameters is `norm_first = True` since the original plain transformer architecture had difficulty converging, but it was later found that doing normalization *before* multiheaded attention stabilizes learning (no heatup with increasing learning rate needed in the beginning of training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "0c81cef0-bf48-4398-bd5a-8b0b58fee38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p,\n",
    "        layer_norm_eps\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Transformer initialized with user specs\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model = d_model,\n",
    "            nhead = nhead,\n",
    "            num_encoder_layers = num_encoder_layers,\n",
    "            num_decoder_layers = num_decoder_layers,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout_p,\n",
    "            layer_norm_eps = layer_norm_eps,\n",
    "            norm_first = True\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "    ):\n",
    "        # Transformer assumes that src & tgt structure is (seq_length, batch_num, feat_dim)\n",
    "        out = self.transformer(src, tgt)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a77195-7686-4afe-86d0-d0939f274c6f",
   "metadata": {},
   "source": [
    "Let's define SimpleTransformer with sminimal settings. Everything is one, but to make the linear feedforward part to have any meaning, we assign 8 neurons there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "f2403825-124d-4ca9-abbd-c05bfb0d88a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 88 parameters (88 trainable)\n"
     ]
    }
   ],
   "source": [
    "model = SimplestTransformer(d_model = 1, nhead = 1, num_encoder_layers = 1, num_decoder_layers = 1, dim_feedforward = 8, dropout_p = 0.1, layer_norm_eps = 1e-05)\n",
    "\n",
    "#print(model)\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters())} parameters ({sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d9f6d-8be9-4db9-a620-9a1941593ab9",
   "metadata": {},
   "source": [
    "**Training data**\n",
    "\n",
    "Let's start with two symbols only:\n",
    "\n",
    " * 0\n",
    " * 1\n",
    "\n",
    "The two symbols can be used in binary sequences. The simplest task is perhaps to continue the sequences. The training data is\n",
    "\n",
    " * X: 0, 0, 0 $\\rightarrow$ Y: 0, 0, 0\n",
    " * X: 1, 1, 1 $\\rightarrow$ Y: 1, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "1a28adb4-aea7-47da-adc1-af24b648f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data1(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "    length = 3\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i in range(n // 2):\n",
    "        X = np.ones(length)\n",
    "        y = np.ones(length)\n",
    "        data.append([X, y])\n",
    "\n",
    "    for i in range(n // 2):\n",
    "        X = np.zeros(length)\n",
    "        y = np.zeros(length)\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7a11d-af1e-467a-a0d5-ff551667bb83",
   "metadata": {},
   "source": [
    "Run several times to make sure both types of sequences are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "65e74ca4-1d13-4297-8690-954df893543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n",
      "[0. 0. 0.]\n",
      "[0. 0. 0.]\n",
      "[1. 1. 1.]\n",
      "[1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "tr_data = generate_data1(100)\n",
    "\n",
    "print(tr_data[0][0])\n",
    "print(tr_data[0][1])\n",
    "\n",
    "print(tr_data[1][0])\n",
    "print(tr_data[1][1])\n",
    "\n",
    "print(tr_data[2][0])\n",
    "print(tr_data[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2f1c1-9afb-4b26-a4fa-dc45ae563863",
   "metadata": {},
   "source": [
    "Form training matrices of ```Seq len x Num samples x Feat dim```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "d3f049f7-14e1-4ad8-a9a6-0da5cba7e6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 100, 1])\n",
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([1., 1., 1.])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "X_tr = torch.empty((len(tr_data[0][0]),len(tr_data),1))\n",
    "Y_tr = torch.empty((len(tr_data[0][1]),len(tr_data),1))\n",
    "for ids, s in enumerate(tr_data):\n",
    "    X_tr[:,ids,0] = torch.from_numpy(s[0])\n",
    "    Y_tr[:,ids,0] = torch.from_numpy(s[1])\n",
    "print(X_tr.shape)\n",
    "\n",
    "print(X_tr[:,0,0])\n",
    "print(Y_tr[:,0,0])\n",
    "\n",
    "print(X_tr[:,1,0])\n",
    "print(Y_tr[:,1,0])\n",
    "\n",
    "print(X_tr[:,2,0])\n",
    "print(Y_tr[:,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "id": "2fb250fa-8678-49e6-8292-936ceb1e94d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 0 training loss 0.5 (lr=0.01)\n",
      "   Epoch 100 training loss 0.2500151991844177 (lr=0.01)\n",
      "   Epoch 200 training loss 0.25 (lr=0.01)\n",
      "   Epoch 300 training loss 0.25 (lr=0.01)\n",
      "   Epoch 400 training loss 0.25 (lr=0.01)\n",
      "   Epoch 500 training loss 0.25 (lr=0.01)\n",
      "   Epoch 600 training loss 0.25 (lr=0.01)\n",
      "   Epoch 700 training loss 0.25 (lr=0.01)\n",
      "   Epoch 800 training loss 0.25 (lr=0.01)\n",
      "   Epoch 900 training loss 0.25 (lr=0.01)\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 1000\n",
    "loss_mse = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[1000], gamma=0.1)\n",
    "model.train()\n",
    "for n in range(num_of_epochs):\n",
    "    running_loss = 0.0\n",
    "    Y_in = Y_tr[:-1,:,:]\n",
    "    Y_out = Y_tr[1:,:,:]\n",
    "    Y_pred = model(X_tr,Y_in)\n",
    "    loss = loss_mse(Y_pred,Y_out)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    if n % 100 == 0:\n",
    "        print(f'   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "a9fde465-b73c-4f01-928c-74b55db0b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_sequence, max_length=16, SOS_token=2, EOS_token=3, EOS_plus = 2):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    y_input = torch.tensor([SOS_token], dtype=torch.long)\n",
    "\n",
    "    sos_found = False\n",
    "    for _ in range(max_length):\n",
    "        #print(f'X={input_sequence} Y={y_input}')\n",
    "\n",
    "        # Get prediction\n",
    "        pred = model(input_sequence, y_input)\n",
    "\n",
    "        pred_tokens = torch.argmax(pred, dim=1)\n",
    "\n",
    "        y_input = torch.cat((torch.tensor([SOS_token], dtype=torch.long), pred_tokens), dim=0)\n",
    "\n",
    "        if pred_tokens[-1] == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7fb764-7ac8-43df-be4e-4561000f0d98",
   "metadata": {},
   "source": [
    "Let's test what the model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "32bc7019-7a16-4727-b1b6-ca222bc96c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor([0.5000, 0.5000], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(X_tr[:,40,0])\n",
    "print(Y_pred[:,40,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc85cb-a1aa-4a4f-85d4-6b6d4c98a527",
   "metadata": {},
   "source": [
    "**Findings:** The Transformer architecture is suitable for providing information about the most likely next token given the input sequence $\\mathbf{X}$ and the current output sequence $\\mathbf{Y}$. However, it the Transformer output itself is used as the next output sequence token, then it cannot do else but provide a single token that minimizes the mean squared error (MSE). In the case of equal amount of '1' and '0' as output it learns to predict 0.5 that is between the two values. You can change the values, e.g., to '0' and '10' to validate this finding.\n",
    "\n",
    "To improve SimplestTransformer we must introduce a mapping from 'tokens' (sequence symbols) to an internal representation that carries out information about the best next token, and then again back to tokens. These two steps are called *embedding* and *unembedding*. They convert every token to a token specific vector (of the length `d_model`) and back. Embedding is a table lookup from 1 to model dimensional vector, but the vector representation can be optimized during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd747f9-495c-4eb8-a045-e087f36a8d59",
   "metadata": {},
   "source": [
    "## SimpleTransformer - Add token embedding and un-embedding\n",
    "\n",
    "At first, let's turn from integers to talk about *tokens*. The input and output sequences, $\\textbf{X}$ and $\\textbf{Y}$, are sequences of tokens with unique (integer) Id. Let's use the following tokens next:\n",
    "\n",
    "```\n",
    "Symbol 0 is 0\n",
    "Symbol 1 is 1\n",
    "Symbol <SOS> is 2 // Start of sequence\n",
    "Symbol <EOS> is 3 // End of sequence\n",
    "```\n",
    "\n",
    "The SOS and EOS tokens help the transformer to learn the beginning and end of sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86160ac4-3f86-4d7c-81cb-ca3bc7650149",
   "metadata": {},
   "source": [
    "Generate tokenized data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1fa239-b60d-4d1e-9d85-f2c9f2a6725d",
   "metadata": {},
   "source": [
    "We have the following tokens:\n",
    "\n",
    " *       `0` : 0 \n",
    " *       `1` : 1 \n",
    " * `<SOS>` : 2\n",
    " * `<EOS>` : 3\n",
    "\n",
    "Let's try to generate a single output from a sequence input\n",
    "\n",
    " * input: SOS,0,0,0,EOS  output: SOS,0,EOS\n",
    " * input: SOS,1,1,1,EOS  output: SOS,1,EOS\n",
    "\n",
    "or vice versa\n",
    "\n",
    " * SOS,0,EOS -> SOS,0,0,0,EOS\n",
    " * SOS,1,EOS -> SOS,1,1,1,EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "f48d91e2-58a7-460c-aa92-f14c51c9abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data2(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 0,0,0,0 -> 1 \n",
    "    for i in range(n // 2):\n",
    "        X = np.concatenate((SOS_token, [0, 0, 0], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [1], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1,1,1,1 -> 0 \n",
    "    for i in range(n // 2):\n",
    "        X = np.concatenate((SOS_token, [1, 1, 1], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [0], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "126b6ed9-e129-4d7e-a6fd-4ca09cea54f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data3(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 1 -> 0,0,0,0 \n",
    "    for i in range(n // 2):\n",
    "        y = np.concatenate((SOS_token, [0, 0, 0, 0], EOS_token))\n",
    "        X = np.concatenate((SOS_token, [1], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1 -> 1,1,1,1 \n",
    "    for i in range(n // 2):\n",
    "        y = np.concatenate((SOS_token, [1, 1, 1, 1], EOS_token))\n",
    "        X = np.concatenate((SOS_token, [0], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "1bfa817a-8a2c-46f4-a564-193875af1d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 3]\n",
      "[2 0 0 0 0 3]\n",
      "[2 0 3]\n",
      "[2 1 1 1 1 3]\n",
      "[2 0 3]\n",
      "[2 1 1 1 1 3]\n"
     ]
    }
   ],
   "source": [
    "tr_data = generate_data3(100)\n",
    "\n",
    "print(tr_data[0][0])\n",
    "print(tr_data[0][1])\n",
    "\n",
    "print(tr_data[1][0])\n",
    "print(tr_data[1][1])\n",
    "\n",
    "print(tr_data[2][0])\n",
    "print(tr_data[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa3dc5-aeaa-498f-bf8a-c1a20b73b073",
   "metadata": {},
   "source": [
    "Generate training data tensors. Note that now we do not generate extra dimension (seq len x num samples x 1) for the tokens that are one-dimensional since embedding will add the requested number of dimensions (seq len x num samples x num feats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "a8f91756-2dd4-4dbb-850f-fa4f95e78ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 100])\n",
      "tensor([2., 1., 3.])\n",
      "tensor([2., 0., 0., 0., 0., 3.])\n",
      "tensor([2., 0., 3.])\n",
      "tensor([2., 1., 1., 1., 1., 3.])\n",
      "tensor([2., 0., 3.])\n",
      "tensor([2., 1., 1., 1., 1., 3.])\n"
     ]
    }
   ],
   "source": [
    "X_tr = torch.empty((len(tr_data[0][0]),len(tr_data)))\n",
    "Y_tr = torch.empty((len(tr_data[0][1]),len(tr_data)))\n",
    "for ids, s in enumerate(tr_data):\n",
    "    X_tr[:,ids] = torch.from_numpy(s[0])\n",
    "    Y_tr[:,ids] = torch.from_numpy(s[1])\n",
    "print(X_tr.shape)\n",
    "\n",
    "print(X_tr[:,0])\n",
    "print(Y_tr[:,0])\n",
    "\n",
    "print(X_tr[:,1])\n",
    "print(Y_tr[:,1])\n",
    "\n",
    "print(X_tr[:,2])\n",
    "print(Y_tr[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce352c8-a925-442d-86b7-30b34994853c",
   "metadata": {},
   "source": [
    "Tokenization does not help as such, but for each token a special *embedding vector* can be generated. Embedding vector may carry more information than a single value.\n",
    "\n",
    "**Note:** Unembedding is actually performed by a linear layer that converts the embedding vector to a vector with the same length as the number of tokens. Now, the next token selection becomes a classification task. We assume that the unembedding vector contains probabilities of each token. By using the *cross-entropy error* as loss function we can optimize this mapping. Note that [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) performs the soft-max operation internally and therefore it does not need to be part of the Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "10bede6b-d8d9-4549-a437-731cba3e17fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p,\n",
    "        layer_norm_eps\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embedding layer - this takes care of converting integer ids to vectors\n",
    "        self.embedding = nn.Embedding(num_tokens, d_model)\n",
    "\n",
    "        # Token \"unembedding\" to one-hot encoded token vector\n",
    "        self.unembedding = nn.Linear(d_model, num_tokens)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model = d_model,\n",
    "            nhead = nhead,\n",
    "            num_encoder_layers = num_encoder_layers,\n",
    "            num_decoder_layers = num_decoder_layers,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout_p,\n",
    "            layer_norm_eps = layer_norm_eps,\n",
    "            norm_first = True\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "    ):\n",
    "        # Note: src & tgt default size is (seq_length, batch_num, feat_dim)\n",
    "\n",
    "        # Token embedding\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)        \n",
    "\n",
    "        # Transformer output\n",
    "        out = self.transformer(src, tgt)\n",
    "        out = self.unembedding(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "6fde67d1-63e4-4aab-bf84-a423167c0017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1332 parameters (1332 trainable)\n"
     ]
    }
   ],
   "source": [
    "model = SimpleTransformer(num_tokens = 4, d_model = 8, nhead = 1, num_encoder_layers = 1, num_decoder_layers = 1, dim_feedforward = 8, dropout_p = 0.1, layer_norm_eps = 1e-05)\n",
    "\n",
    "#print(model)\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters())} parameters ({sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "b8a9f198-fc4a-4f6d-9a9a-50aeb88b0b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 0 training loss 1.340372920036316 (lr=0.01)\n",
      "   Epoch 10 training loss 0.8127036094665527 (lr=0.01)\n",
      "   Epoch 20 training loss 0.6399750709533691 (lr=0.01)\n",
      "   Epoch 30 training loss 0.5244415402412415 (lr=0.01)\n",
      "   Epoch 40 training loss 0.48644545674324036 (lr=0.01)\n",
      "   Epoch 50 training loss 0.4820917844772339 (lr=0.01)\n",
      "   Epoch 60 training loss 0.47530338168144226 (lr=0.01)\n",
      "   Epoch 70 training loss 0.46358826756477356 (lr=0.01)\n",
      "   Epoch 80 training loss 0.46134352684020996 (lr=0.01)\n",
      "   Epoch 90 training loss 0.4614886939525604 (lr=0.01)\n",
      "   Epoch 100 training loss 0.4553680121898651 (lr=0.001)\n",
      "   Epoch 110 training loss 0.4542762339115143 (lr=0.001)\n",
      "   Epoch 120 training loss 0.45668914914131165 (lr=0.001)\n",
      "   Epoch 130 training loss 0.45848241448402405 (lr=0.001)\n",
      "   Epoch 140 training loss 0.4633973240852356 (lr=0.001)\n",
      "   Epoch 150 training loss 0.46021488308906555 (lr=0.001)\n",
      "   Epoch 160 training loss 0.4602748155593872 (lr=0.001)\n",
      "   Epoch 170 training loss 0.45115283131599426 (lr=0.001)\n",
      "   Epoch 180 training loss 0.4618590474128723 (lr=0.001)\n",
      "   Epoch 190 training loss 0.45415380597114563 (lr=0.001)\n",
      "   Epoch 200 training loss 0.4577707350254059 (lr=0.001)\n",
      "   Epoch 210 training loss 0.4566449522972107 (lr=0.001)\n",
      "   Epoch 220 training loss 0.45640408992767334 (lr=0.001)\n",
      "   Epoch 230 training loss 0.4548245966434479 (lr=0.001)\n",
      "   Epoch 240 training loss 0.45898619294166565 (lr=0.001)\n",
      "   Epoch 250 training loss 0.45930859446525574 (lr=0.001)\n",
      "   Epoch 260 training loss 0.4544047713279724 (lr=0.001)\n",
      "   Epoch 270 training loss 0.4564797878265381 (lr=0.001)\n",
      "   Epoch 280 training loss 0.4545906186103821 (lr=0.001)\n",
      "   Epoch 290 training loss 0.4556931257247925 (lr=0.001)\n",
      "   Epoch 300 training loss 0.4558729827404022 (lr=0.001)\n",
      "   Epoch 310 training loss 0.459673672914505 (lr=0.001)\n",
      "   Epoch 320 training loss 0.4585474729537964 (lr=0.001)\n",
      "   Epoch 330 training loss 0.4576200544834137 (lr=0.001)\n",
      "   Epoch 340 training loss 0.4561530649662018 (lr=0.001)\n",
      "   Epoch 350 training loss 0.45594871044158936 (lr=0.001)\n",
      "   Epoch 360 training loss 0.45573192834854126 (lr=0.001)\n",
      "   Epoch 370 training loss 0.45620906352996826 (lr=0.001)\n",
      "   Epoch 380 training loss 0.460002601146698 (lr=0.001)\n",
      "   Epoch 390 training loss 0.4529871642589569 (lr=0.001)\n",
      "   Epoch 400 training loss 0.45942258834838867 (lr=0.001)\n",
      "   Epoch 410 training loss 0.4578333795070648 (lr=0.001)\n",
      "   Epoch 420 training loss 0.45481449365615845 (lr=0.001)\n",
      "   Epoch 430 training loss 0.45315608382225037 (lr=0.001)\n",
      "   Epoch 440 training loss 0.4523077607154846 (lr=0.001)\n",
      "   Epoch 450 training loss 0.4566851556301117 (lr=0.001)\n",
      "   Epoch 460 training loss 0.46392324566841125 (lr=0.001)\n",
      "   Epoch 470 training loss 0.45397257804870605 (lr=0.001)\n",
      "   Epoch 480 training loss 0.45416587591171265 (lr=0.001)\n",
      "   Epoch 490 training loss 0.4570156931877136 (lr=0.001)\n",
      "   Epoch 500 training loss 0.4515078067779541 (lr=0.001)\n",
      "   Epoch 510 training loss 0.4596318006515503 (lr=0.001)\n",
      "   Epoch 520 training loss 0.4546595513820648 (lr=0.001)\n",
      "   Epoch 530 training loss 0.45181477069854736 (lr=0.001)\n",
      "   Epoch 540 training loss 0.4572521448135376 (lr=0.001)\n",
      "   Epoch 550 training loss 0.4531552493572235 (lr=0.001)\n",
      "   Epoch 560 training loss 0.4567238390445709 (lr=0.001)\n",
      "   Epoch 570 training loss 0.451526939868927 (lr=0.001)\n",
      "   Epoch 580 training loss 0.4552965462207794 (lr=0.001)\n",
      "   Epoch 590 training loss 0.4528811275959015 (lr=0.001)\n",
      "   Epoch 600 training loss 0.4526960849761963 (lr=0.001)\n",
      "   Epoch 610 training loss 0.4549010097980499 (lr=0.001)\n",
      "   Epoch 620 training loss 0.4498993158340454 (lr=0.001)\n",
      "   Epoch 630 training loss 0.4538191854953766 (lr=0.001)\n",
      "   Epoch 640 training loss 0.4539371728897095 (lr=0.001)\n",
      "   Epoch 650 training loss 0.4550970792770386 (lr=0.001)\n",
      "   Epoch 660 training loss 0.4511335790157318 (lr=0.001)\n",
      "   Epoch 670 training loss 0.4564407765865326 (lr=0.001)\n",
      "   Epoch 680 training loss 0.45302483439445496 (lr=0.001)\n",
      "   Epoch 690 training loss 0.44954726099967957 (lr=0.001)\n",
      "   Epoch 700 training loss 0.45354875922203064 (lr=0.001)\n",
      "   Epoch 710 training loss 0.45383748412132263 (lr=0.001)\n",
      "   Epoch 720 training loss 0.4487352669239044 (lr=0.001)\n",
      "   Epoch 730 training loss 0.4530163109302521 (lr=0.001)\n",
      "   Epoch 740 training loss 0.45509225130081177 (lr=0.001)\n",
      "   Epoch 750 training loss 0.45635128021240234 (lr=0.001)\n",
      "   Epoch 760 training loss 0.4501042664051056 (lr=0.001)\n",
      "   Epoch 770 training loss 0.45120149850845337 (lr=0.001)\n",
      "   Epoch 780 training loss 0.4537549614906311 (lr=0.001)\n",
      "   Epoch 790 training loss 0.45413029193878174 (lr=0.001)\n",
      "   Epoch 800 training loss 0.4538072645664215 (lr=0.001)\n",
      "   Epoch 810 training loss 0.45402371883392334 (lr=0.001)\n",
      "   Epoch 820 training loss 0.455992728471756 (lr=0.001)\n",
      "   Epoch 830 training loss 0.45099562406539917 (lr=0.001)\n",
      "   Epoch 840 training loss 0.4518128037452698 (lr=0.001)\n",
      "   Epoch 850 training loss 0.4541967213153839 (lr=0.001)\n",
      "   Epoch 860 training loss 0.45084255933761597 (lr=0.001)\n",
      "   Epoch 870 training loss 0.4504227042198181 (lr=0.001)\n",
      "   Epoch 880 training loss 0.4499547779560089 (lr=0.001)\n",
      "   Epoch 890 training loss 0.45349037647247314 (lr=0.001)\n",
      "   Epoch 900 training loss 0.45243850350379944 (lr=0.001)\n",
      "   Epoch 910 training loss 0.45261111855506897 (lr=0.001)\n",
      "   Epoch 920 training loss 0.45222970843315125 (lr=0.001)\n",
      "   Epoch 930 training loss 0.451589971780777 (lr=0.001)\n",
      "   Epoch 940 training loss 0.45277678966522217 (lr=0.001)\n",
      "   Epoch 950 training loss 0.4522283673286438 (lr=0.001)\n",
      "   Epoch 960 training loss 0.45012667775154114 (lr=0.001)\n",
      "   Epoch 970 training loss 0.44915124773979187 (lr=0.001)\n",
      "   Epoch 980 training loss 0.45600658655166626 (lr=0.001)\n",
      "   Epoch 990 training loss 0.453648179769516 (lr=0.001)\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 1000\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[100], gamma=0.1)\n",
    "model.train()\n",
    "for n in range(num_of_epochs):\n",
    "    running_loss = 0.0\n",
    "    X_in = X_tr.long()\n",
    "    Y_in = Y_tr[:-1,:].long()\n",
    "    Y_out = Y_tr[1:,:].long()\n",
    "    Y_pred = model(X_in,Y_in)\n",
    "    # seq len x num samples => num samples x seq len\n",
    "    Y_out = Y_out.permute(1,0)\n",
    "    # seq len x num samples x token one hot => num samples x token one hot x seq len\n",
    "    Y_pred = Y_pred.permute(1, 2, 0)\n",
    "    loss = loss_fn(Y_pred,Y_out)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    if n % 10 == 0:\n",
    "        print(f'   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "da8dbfc1-96a2-43bf-a4cc-48bd07eb4ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input: [1, 1, 1]\n",
      "Continuation: [1]\n",
      "\n",
      "Example 1\n",
      "Input: [0, 0, 0]\n",
      "Continuation: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Example 2\n",
      "Input: [1]\n",
      "Continuation: [1]\n",
      "\n",
      "Example 3\n",
      "Input: [0]\n",
      "Continuation: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([2, 1, 1, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 0, 0, 0, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 0, 3], dtype=torch.long)\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99557f0-1efe-4932-a12f-18984f30ba71",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "\n",
    "Interestingly, the ```SimpleTransformer``` architecture learns to map long sequences to short, e.g., 0,0,0,0 -> 1 and 1,1,1,1 -> 0 , but not short sequences to long, e.g., 0 -> 1,1,1,1 and 1 -> 0,0,0,0.\n",
    "\n",
    "The problem is that we train it incorrectly.\n",
    "\n",
    "SimpleTransformer is trained with full sequences\n",
    "\n",
    " * SOS,1,EOS -> SOS,0,0,0,0,EOS\n",
    " * SOS,0,EOS -> SOS,1,1,1,1,EOS\n",
    "\n",
    " BUT during inference it only knows the input and what is generated so far\n",
    "\n",
    " * (SOS,1,EOS),(SOS) -> 0\n",
    " * (SOS,1,EOS),(SOS,0) -> 0,0\n",
    " * (SOS,1,EOS),(SOS,0,0) -> 0,0,0\n",
    " * (SOS,1,EOS),(SOS,0,0,0) -> 0,0,0,0\n",
    " * (SOS,1,EOS),(SOS,0,0,0,0) -> 0,0,0,0,EOS\n",
    "\n",
    "This means that inference is different from training, and especially in the short-to-long case.\n",
    "\n",
    "This can be fixed by **masking** the future outputs during taining. After this little extension the transformer is already pretty powerful model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7761d-f0df-4d33-8dd4-86a10043aa2f",
   "metadata": {},
   "source": [
    "## SimpleTransformerPlus - Masking the future outputs during training\n",
    "\n",
    "```SimpleTransformer```was able to learn to produce a single output from multiple, but not multiple outputs from single. The main reason for that is that there is discrepancy between the training data and test data. During training it was aware of the full future, i.e., its task was to learn to produce a missing token in the following cases\n",
    "\n",
    " * SOS,1,EOS -> SOS,?,0,0,,EOS\n",
    " * SOS,1,EOS -> SOS,0,?,0,EOS\n",
    " * SOS,1,EOS -> SOS,0,0,?,EOS\n",
    " * SOS,1,EOS -> SOS,0,0,0,?\n",
    "\n",
    "while during testin the 'future' outputs are not known and the problem is actually\n",
    "\n",
    "\n",
    " * SOS,1,EOS -> SOS,?\n",
    " * SOS,1,EOS -> SOS,0,?\n",
    " * SOS,1,EOS -> SOS,0,0,?\n",
    " * SOS,1,EOS -> SOS,0,0,0,?\n",
    "\n",
    "which is different from the training. Note that in the many-to-one case training and test cases are almost identical and that is why it worked.\n",
    "\n",
    "**Solution:** During training we need to mask the future outputs. There is a pre-made Torch function for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "423ce223-4f6b-47a5-9421-ada9845ef096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 10\n",
    "tgt_mask = nn.Transformer.generate_square_subsequent_mask(sequence_length)\n",
    "print(tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd6a41-7c34-4d51-9154-263939ebdd88",
   "metadata": {},
   "source": [
    "Let's define ```SimpleTransformerPlus``` that supports masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "bd2b5720-17f3-4df4-9991-337049b735fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformerPlus(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p,\n",
    "        layer_norm_eps\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embedding layer - this takes care of converting integer to vectors\n",
    "        self.embedding = nn.Embedding(num_tokens, d_model)\n",
    "\n",
    "        # Token \"unembedding\" to one-hot token vector\n",
    "        self.unembedding = nn.Linear(d_model, num_tokens)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model = d_model,\n",
    "            nhead = nhead,\n",
    "            num_encoder_layers = num_encoder_layers,\n",
    "            num_decoder_layers = num_decoder_layers,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout_p,\n",
    "            layer_norm_eps = layer_norm_eps,\n",
    "            norm_first = True\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "        tgt_mask = None\n",
    "    ):\n",
    "        # Note: src & tgt default size is (seq_length, batch_num, feat_dim)\n",
    "\n",
    "        # Token embedding\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)        \n",
    "\n",
    "        # Transformer output\n",
    "        out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        out = self.unembedding(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "3a7985bf-f5a8-41cf-9054-fef82a8ac66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1332 parameters (1332 trainable)\n"
     ]
    }
   ],
   "source": [
    "model = SimpleTransformerPlus(num_tokens = 4, d_model = 8, nhead = 1, num_encoder_layers = 1, num_decoder_layers = 1, dim_feedforward = 8, dropout_p = 0.1, layer_norm_eps = 1e-05)\n",
    "\n",
    "#print(model)\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters())} parameters ({sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "9dc01f14-e1a4-453b-82fb-ee685412af9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 0 training loss 1.3542481660842896 (lr=0.01)\n",
      "   Epoch 100 training loss 0.4061003029346466 (lr=0.01)\n",
      "   Epoch 200 training loss 0.3027517795562744 (lr=0.01)\n",
      "   Epoch 300 training loss 0.23316869139671326 (lr=0.01)\n",
      "   Epoch 400 training loss 0.266167551279068 (lr=0.01)\n",
      "   Epoch 500 training loss 0.24914109706878662 (lr=0.001)\n",
      "   Epoch 600 training loss 0.20288799703121185 (lr=0.001)\n",
      "   Epoch 700 training loss 0.2525886595249176 (lr=0.001)\n",
      "   Epoch 800 training loss 0.19239892065525055 (lr=0.001)\n",
      "   Epoch 900 training loss 0.20035260915756226 (lr=0.001)\n",
      "   Epoch 1000 training loss 0.17571181058883667 (lr=0.001)\n",
      "   Epoch 1100 training loss 0.20941556990146637 (lr=0.001)\n",
      "   Epoch 1200 training loss 0.20730094611644745 (lr=0.001)\n",
      "   Epoch 1300 training loss 0.18186041712760925 (lr=0.001)\n",
      "   Epoch 1400 training loss 0.216546431183815 (lr=0.001)\n",
      "   Epoch 1500 training loss 0.1846693605184555 (lr=0.001)\n",
      "   Epoch 1600 training loss 0.19660606980323792 (lr=0.001)\n",
      "   Epoch 1700 training loss 0.2294130027294159 (lr=0.001)\n",
      "   Epoch 1800 training loss 0.19157074391841888 (lr=0.001)\n",
      "   Epoch 1900 training loss 0.1971454620361328 (lr=0.001)\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 2000\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[500], gamma=0.1)\n",
    "model.train()\n",
    "for n in range(num_of_epochs):\n",
    "    running_loss = 0.0\n",
    "    X_in = X_tr.long()\n",
    "    Y_in = Y_tr[:-1,:].long()\n",
    "    Y_out = Y_tr[1:,:].long()\n",
    "\n",
    "    # Get mask to mask out the next words\n",
    "    sequence_length = Y_in.size(0)\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(sequence_length)\n",
    "    \n",
    "    Y_pred = model(X_in,Y_in, tgt_mask = tgt_mask)\n",
    "\n",
    "    # seq len x num samples => num samples x seq len\n",
    "    Y_out = Y_out.permute(1,0)\n",
    "    # seq len x num samples x token one hot => num samples x token one hot x seq len    \n",
    "    Y_pred = Y_pred.permute(1, 2, 0)\n",
    "    loss = loss_fn(Y_pred,Y_out)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    if n % 100 == 0:\n",
    "        print(f'   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc1c45-b4e9-4ea0-be1e-4a40bb47466b",
   "metadata": {},
   "source": [
    "**Inference** must be updated as well for nn.Transformer since it re-produces all outputs sequentially, but without masking it \"sees\" the future that was not anymore allowed in training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "498d0f25-7161-427e-aa50-94ce6863e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_sequence, max_length=16, SOS_token=2, EOS_token=3, EOS_plus = 2):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    y_input = torch.tensor([SOS_token], dtype=torch.long)\n",
    "\n",
    "    sos_found = False\n",
    "    for _ in range(max_length):\n",
    "        #print(f'X={input_sequence} Y={y_input}')\n",
    "\n",
    "        sequence_length = len(y_input)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(sequence_length)\n",
    "    \n",
    "        pred = model(input_sequence, y_input, tgt_mask = tgt_mask)\n",
    "\n",
    "        pred_tokens = torch.argmax(pred, dim=1)\n",
    "\n",
    "        y_input = torch.cat((torch.tensor([SOS_token], dtype=torch.long), pred_tokens), dim=0)\n",
    "\n",
    "        if pred_tokens[-1] == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "a5c5d8f3-f3bb-4516-b682-0960a10802b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input: [1, 1, 1]\n",
      "Continuation: [0, 0, 0, 0]\n",
      "\n",
      "Example 1\n",
      "Input: [0, 0, 0]\n",
      "Continuation: [1, 1, 1, 1]\n",
      "\n",
      "Example 2\n",
      "Input: [1]\n",
      "Continuation: [0, 0, 0, 0]\n",
      "\n",
      "Example 3\n",
      "Input: [0]\n",
      "Continuation: [1, 1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([2, 1, 1, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 0, 0, 0, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 0, 3], dtype=torch.long)\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffde892-abee-410f-a663-e0228ffb5edf",
   "metadata": {},
   "source": [
    "Let's try something different - how about learning how to continue a sequence\n",
    "\n",
    "* sos,0,1,0,1,eos $\\rightarrow$ sos,0,1,0,1,eos\n",
    "* sos,1,0,1,0,eos $\\rightarrow$ sos,1,0,1,0,eos\n",
    "\n",
    "The above looks simple enough so that ```SimpleTransformerPlus```should be able to learn it now that it can process sequences correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c176c421-0018-42fc-a4c0-abf94037554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data4(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 1 -> 0,0,0,0 \n",
    "    for i in range(n // 2):\n",
    "        X = np.concatenate((SOS_token, [0, 1, 0, 1], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [0, 1, 0, 1], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1 -> 1,1,1,1 \n",
    "    for i in range(n // 2):\n",
    "        X = np.concatenate((SOS_token, [1, 0, 1, 0], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [1, 0, 1, 0], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b925b448-f20c-4c35-a38a-02a1a6cd7051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1 0 3]\n",
      "[2 1 0 1 0 3]\n",
      "[2 1 0 1 0 3]\n",
      "[2 1 0 1 0 3]\n",
      "[2 0 1 0 1 3]\n",
      "[2 0 1 0 1 3]\n"
     ]
    }
   ],
   "source": [
    "tr_data = generate_data4(100)\n",
    "\n",
    "print(tr_data[0][0])\n",
    "print(tr_data[0][1])\n",
    "\n",
    "print(tr_data[1][0])\n",
    "print(tr_data[1][1])\n",
    "\n",
    "print(tr_data[2][0])\n",
    "print(tr_data[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce60c673-0b1f-405a-8c8d-d6901c81f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 100])\n",
      "tensor([2., 1., 0., 1., 0., 3.])\n",
      "tensor([2., 1., 0., 1., 0., 3.])\n",
      "tensor([2., 1., 0., 1., 0., 3.])\n",
      "tensor([2., 1., 0., 1., 0., 3.])\n",
      "tensor([2., 0., 1., 0., 1., 3.])\n",
      "tensor([2., 0., 1., 0., 1., 3.])\n"
     ]
    }
   ],
   "source": [
    "X_tr = torch.empty((len(tr_data[0][0]),len(tr_data)))\n",
    "Y_tr = torch.empty((len(tr_data[0][1]),len(tr_data)))\n",
    "for ids, s in enumerate(tr_data):\n",
    "    X_tr[:,ids] = torch.from_numpy(s[0])\n",
    "    Y_tr[:,ids] = torch.from_numpy(s[1])\n",
    "print(X_tr.shape)\n",
    "\n",
    "print(X_tr[:,0])\n",
    "print(Y_tr[:,0])\n",
    "\n",
    "print(X_tr[:,1])\n",
    "print(Y_tr[:,1])\n",
    "\n",
    "print(X_tr[:,2])\n",
    "print(Y_tr[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "289add26-c228-46b2-8bdf-5151e5953ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1332 parameters (1332 trainable)\n"
     ]
    }
   ],
   "source": [
    "model = SimpleTransformerPlus(num_tokens = 4, d_model = 8, nhead = 1, num_encoder_layers = 1, num_decoder_layers = 1, dim_feedforward = 8, dropout_p = 0.1, layer_norm_eps = 1e-05)\n",
    "\n",
    "#print(model)\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters())} parameters ({sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "e056bbb0-f801-4290-bf7b-07adcd961868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 0 training loss 1.317150354385376 (lr=0.01)\n",
      "   Epoch 100 training loss 0.3775438368320465 (lr=0.01)\n",
      "   Epoch 200 training loss 0.40597498416900635 (lr=0.01)\n",
      "   Epoch 300 training loss 0.21399351954460144 (lr=0.01)\n",
      "   Epoch 400 training loss 0.5730879902839661 (lr=0.01)\n",
      "   Epoch 500 training loss 0.4395396411418915 (lr=0.001)\n",
      "   Epoch 600 training loss 0.27175191044807434 (lr=0.001)\n",
      "   Epoch 700 training loss 0.2718021273612976 (lr=0.001)\n",
      "   Epoch 800 training loss 0.24456676840782166 (lr=0.001)\n",
      "   Epoch 900 training loss 0.24476777017116547 (lr=0.001)\n",
      "   Epoch 1000 training loss 0.22980175912380219 (lr=0.001)\n",
      "   Epoch 1100 training loss 0.20475421845912933 (lr=0.001)\n",
      "   Epoch 1200 training loss 0.2261151522397995 (lr=0.001)\n",
      "   Epoch 1300 training loss 0.2475406527519226 (lr=0.001)\n",
      "   Epoch 1400 training loss 0.2095976322889328 (lr=0.001)\n",
      "   Epoch 1500 training loss 0.2383974939584732 (lr=0.001)\n",
      "   Epoch 1600 training loss 0.21962140500545502 (lr=0.001)\n",
      "   Epoch 1700 training loss 0.23983775079250336 (lr=0.001)\n",
      "   Epoch 1800 training loss 0.20759879052639008 (lr=0.001)\n",
      "   Epoch 1900 training loss 0.1926451325416565 (lr=0.001)\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 2000\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[500], gamma=0.1)\n",
    "model.train()\n",
    "for n in range(num_of_epochs):\n",
    "    running_loss = 0.0\n",
    "    X_in = X_tr.long()\n",
    "    Y_in = Y_tr[:-1,:].long()\n",
    "    Y_out = Y_tr[1:,:].long()\n",
    "\n",
    "    # Get mask to mask out the next words\n",
    "    sequence_length = Y_in.size(0)\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(sequence_length)\n",
    "    \n",
    "    Y_pred = model(X_in,Y_in, tgt_mask = tgt_mask)\n",
    "\n",
    "    # seq len x num samples => num samples x seq len\n",
    "    Y_out = Y_out.permute(1,0)\n",
    "    # seq len x num samples x token one hot => num samples x token one hot x seq len    \n",
    "    Y_pred = Y_pred.permute(1, 2, 0)\n",
    "    loss = loss_fn(Y_pred,Y_out)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    if n % 100 == 0:\n",
    "        print(f'   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "dbb19d7a-6696-45df-9275-185847574c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input: [0, 1, 0, 1]\n",
      "Continuation: [1, 0]\n",
      "\n",
      "Example 1\n",
      "Input: [1, 0, 1, 0]\n",
      "Continuation: [1, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([2, 0, 1, 0, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 1, 0, 1, 0, 3], dtype=torch.long),\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd177d52-f21a-49fc-bf8e-fc14c06ba497",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "\n",
    "Despite of its power ```SimpleTransformer``` to predict future from past tokes, is not able to learn simple sequence continuation such as 0,1,0,1 -> 0,1,0,1 and 1,0,1,0 -> 1,0,1,0.\n",
    "\n",
    "The reason is that the embeddings for the tokens 0 and 1 are the same, and summing them up is the same for 0,1,0,1 and 1,0,1,0 (and even for 1100, 1001, 0011) so the transformer does not know what is the correct.\n",
    "\n",
    "In order to fix this problem we need to make Transformer aware about the position of each token. That can be done by\n",
    "\n",
    " * Positional encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e8703-0c6d-4117-8626-c1ce7fc3da24",
   "metadata": {},
   "source": [
    "## ATransformer - Positional encoding is the last missing ingredient\n",
    "\n",
    "Plethora of code snippets can be found for positional encoding so let's pick one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "228605ac-d123-4376-8791-df44cc011280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebe7d4-e428-4d43-84c2-7fd29c12a8eb",
   "metadata": {},
   "source": [
    "The code shows that the encoding is additive so by inputting zero vectors we see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39f78502-1d1b-463f-832d-46121e6477e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "foo_x = torch.zeros((4,1,8))\n",
    "foo_encoding = PositionalEncoding(8)\n",
    "foo_x_enc = foo_encoding(foo_x)\n",
    "\n",
    "plt.plot(np.squeeze(foo_x_enc).t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "265feeff-dddf-407b-a2f5-3c767499cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p,\n",
    "        layer_norm_eps\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embedding layer - this takes care of converting integer to vectors\n",
    "        self.embedding = nn.Embedding(num_tokens, d_model)\n",
    "\n",
    "        # Token \"unembedding\" to one-hot token vector\n",
    "        self.unembedding = nn.Linear(d_model, num_tokens)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoder = PositionalEncoding(d_model=d_model, dropout=dropout_p)\n",
    "\n",
    "        # nn.Transformer that does the magic\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model = d_model,\n",
    "            nhead = nhead,\n",
    "            num_encoder_layers = num_encoder_layers,\n",
    "            num_decoder_layers = num_decoder_layers,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout_p,\n",
    "            layer_norm_eps = layer_norm_eps,\n",
    "            norm_first = True\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "        tgt_mask = None\n",
    "    ):\n",
    "        # Note: src & tgt default size is (seq_length, batch_num, feat_dim)\n",
    "\n",
    "        # Token embedding\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "\n",
    "        # Positional encoding - this is sensitive that data _must_ be seq len x batch num x feat dim\n",
    "        # Inference often misses the batch num\n",
    "        if src.dim() == 2: # seq len x feat dim\n",
    "            src = torch.unsqueeze(src,1) \n",
    "        src = self.positional_encoder(src)\n",
    "        if tgt.dim() == 2: # seq len x feat dim\n",
    "            tgt = torch.unsqueeze(tgt,1) \n",
    "        tgt = self.positional_encoder(tgt)\n",
    "\n",
    "        # Transformer output\n",
    "        out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        out = self.unembedding(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5ba01145-105a-4094-b491-bd713a90b208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1332 parameters (1332 trainable)\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(num_tokens = 4, d_model = 8, nhead = 1, num_encoder_layers = 1, num_decoder_layers = 1, dim_feedforward = 8, dropout_p = 0.1, layer_norm_eps = 1e-05)\n",
    "\n",
    "#print(model)\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters())} parameters ({sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "907a8f23-3fc3-4de5-9489-b7133deb4217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 0 training loss 0.9784363508224487 (lr=0.01)\n",
      "   Epoch 100 training loss 0.10000507533550262 (lr=0.01)\n",
      "   Epoch 200 training loss 0.033111266791820526 (lr=0.01)\n",
      "   Epoch 300 training loss 0.05412794277071953 (lr=0.01)\n",
      "   Epoch 400 training loss 0.012893409468233585 (lr=0.01)\n",
      "   Epoch 500 training loss 0.008361301384866238 (lr=0.001)\n",
      "   Epoch 600 training loss 0.008510332554578781 (lr=0.001)\n",
      "   Epoch 700 training loss 0.0036712519358843565 (lr=0.001)\n",
      "   Epoch 800 training loss 0.010958276689052582 (lr=0.001)\n",
      "   Epoch 900 training loss 0.003900029929354787 (lr=0.001)\n",
      "   Epoch 1000 training loss 0.008949804119765759 (lr=0.001)\n",
      "   Epoch 1100 training loss 0.0043568965047597885 (lr=0.001)\n",
      "   Epoch 1200 training loss 0.003931717947125435 (lr=0.001)\n",
      "   Epoch 1300 training loss 0.011823654174804688 (lr=0.001)\n",
      "   Epoch 1400 training loss 0.012323993258178234 (lr=0.001)\n",
      "   Epoch 1500 training loss 0.02136898972094059 (lr=0.001)\n",
      "   Epoch 1600 training loss 0.0018198979087173939 (lr=0.001)\n",
      "   Epoch 1700 training loss 0.013360584154725075 (lr=0.001)\n",
      "   Epoch 1800 training loss 0.004663780797272921 (lr=0.001)\n",
      "   Epoch 1900 training loss 0.0011523370631039143 (lr=0.001)\n",
      "Final:   Epoch 1999 training loss 0.0022679311223328114 (lr=0.001)\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 2000\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[500], gamma=0.1)\n",
    "model.train()\n",
    "for n in range(num_of_epochs):\n",
    "    running_loss = 0.0\n",
    "    X_in = X_tr.long()\n",
    "    Y_in = Y_tr[:-1,:].long()\n",
    "    Y_out = Y_tr[1:,:].long()\n",
    "\n",
    "    # Get mask to mask out the next words\n",
    "    sequence_length = Y_in.size(0)\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(sequence_length)\n",
    "    \n",
    "    Y_pred = model(X_in,Y_in, tgt_mask = tgt_mask)\n",
    "\n",
    "    # seq len x num samples => num samples x seq len\n",
    "    Y_out = Y_out.permute(1,0)\n",
    "    # seq len x num samples x token one hot => num samples x token one hot x seq len    \n",
    "    Y_pred = Y_pred.permute(1, 2, 0)\n",
    "    loss = loss_fn(Y_pred,Y_out)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    if n % 100 == 0:\n",
    "        print(f'   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')\n",
    "print(f'Final:   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "199ab909-818a-46ae-88ee-13352091ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_sequence, max_length=16, SOS_token=2, EOS_token=3, EOS_plus = 2):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    y_input = torch.tensor([SOS_token], dtype=torch.long)\n",
    "\n",
    "    sos_found = False\n",
    "    for _ in range(max_length):\n",
    "        #print(f'X={input_sequence} Y={y_input}')\n",
    "\n",
    "        sequence_length = len(y_input)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(sequence_length)\n",
    "\n",
    "        pred = model(input_sequence, y_input, tgt_mask = tgt_mask)\n",
    "\n",
    "        # Since the positional encoding implementation the model returns seq len x batch num x feat num\n",
    "        if pred.dim() == 3:\n",
    "            pred = torch.squeeze(pred,1)\n",
    "        \n",
    "        pred_tokens = torch.argmax(pred, dim=1)\n",
    "\n",
    "        y_input = torch.cat((torch.tensor([SOS_token], dtype=torch.long), pred_tokens), dim=0)\n",
    "\n",
    "        if pred_tokens[-1] == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2248cc82-1b41-4384-b7c1-6200bbea3e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input: [0, 1, 0, 1]\n",
      "Continuation: [0, 1, 0, 1]\n",
      "\n",
      "Example 1\n",
      "Input: [1, 0, 1, 0]\n",
      "Continuation: [1, 0, 1, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([2, 0, 1, 0, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 1, 0, 1, 0, 3], dtype=torch.long),\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6baec2-4879-4e4e-974b-d52795d9836f",
   "metadata": {},
   "source": [
    "**Findings** Works like a charm!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbce9c7-e300-40bb-b2a4-2201a7bbe957",
   "metadata": {},
   "source": [
    "## Continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3868fbd8-ad69-4412-924a-695394b36536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
